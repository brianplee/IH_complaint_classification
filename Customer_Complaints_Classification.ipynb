{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IH Complaints/Routine Service Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform imports and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Service Request Number</th>\n",
       "      <th>SR Type</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Merged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-100039399</td>\n",
       "      <td>Complaint</td>\n",
       "      <td>BGG-RUO</td>\n",
       "      <td>Yellow flags for multiple samples Site reporte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-100786893</td>\n",
       "      <td>Complaint</td>\n",
       "      <td>BGG-RUO</td>\n",
       "      <td>Results not appearing in BIDS software Site re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-101810312</td>\n",
       "      <td>Complaint</td>\n",
       "      <td>BGG-RUO</td>\n",
       "      <td>Unable to launch Bids XT software / Life Share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-101810811</td>\n",
       "      <td>Complaint</td>\n",
       "      <td>BGG-RUO</td>\n",
       "      <td>during FDA genotyping training customer tried ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-101887056</td>\n",
       "      <td>Complaint</td>\n",
       "      <td>BGG-RUO</td>\n",
       "      <td>FDA genotyping customer did not have Taq polym...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Service Request Number    SR Type Platform  \\\n",
       "0            1-100039399  Complaint  BGG-RUO   \n",
       "1            1-100786893  Complaint  BGG-RUO   \n",
       "2            1-101810312  Complaint  BGG-RUO   \n",
       "3            1-101810811  Complaint  BGG-RUO   \n",
       "4            1-101887056  Complaint  BGG-RUO   \n",
       "\n",
       "                                              Merged  \n",
       "0  Yellow flags for multiple samples Site reporte...  \n",
       "1  Results not appearing in BIDS software Site re...  \n",
       "2  Unable to launch Bids XT software / Life Share...  \n",
       "3  during FDA genotyping training customer tried ...  \n",
       "4  FDA genotyping customer did not have Taq polym...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('../IH Complaint Machine Learning/IH_combined.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5897"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for missing values and empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Service Request Number    0\n",
       "SR Type                   6\n",
       "Platform                  0\n",
       "Merged                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for NaN\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Service Request Number    0\n",
       "SR Type                   0\n",
       "Platform                  0\n",
       "Merged                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5891"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for empty strings\n",
    "\n",
    "blanks = []\n",
    "for i, s, srtype, plat, merged in df.itertuples():\n",
    "    if type(merged)==str:\n",
    "        if merged.isspace():\n",
    "            blanks.append(i)\n",
    "len(blanks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5790"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(blanks, inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5790"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5897 - 6 - 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Label (SR Type) column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Routine Service    3602\n",
       "Complaint          2188\n",
       "Name: SR Type, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SR Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train & test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Merged']\n",
    "y = df['SR Type']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build pipeline to vectorize text, then train and fit a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline([('tfidf',TfidfVectorizer()),\n",
    "                     ('clf',LinearSVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions and analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = text_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 575  159]\n",
      " [ 115 1062]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RS</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RS</th>\n",
       "      <td>575</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>115</td>\n",
       "      <td>1062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     RS     C\n",
       "RS  575   159\n",
       "C   115  1062"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,predictions))\n",
    "cmdf = pd.DataFrame(confusion_matrix(y_test,predictions),index=[\"RS\",\"C\"],columns=[\"RS\",\"C\"])\n",
    "cmdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "      Complaint       0.83      0.78      0.81       734\n",
      "Routine Service       0.87      0.90      0.89      1177\n",
      "\n",
      "    avg / total       0.86      0.86      0.86      1911\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8566195709052852\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model of made up entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Complaint'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict([\"Unexpected results with cell 1 on Search-Cyte TCS 0.8% lot 024\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Routine Service'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(['Fluidics pressure error with Fluid A container.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index and evaluate mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1891          Complaint\n",
       "1550    Routine Service\n",
       "1049    Routine Service\n",
       "2523    Routine Service\n",
       "156           Complaint\n",
       "Name: SR Type, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Complaint', 'Routine Service', 'Routine Service',\n",
       "       'Routine Service', 'Complaint'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1911\n",
      "1911\n"
     ]
    }
   ],
   "source": [
    "y_test_list = []\n",
    "prediction_list = []\n",
    "for i in y_test:\n",
    "    y_test_list.append(i)\n",
    "for p in predictions:\n",
    "    prediction_list.append(p)\n",
    "    \n",
    "print(len(y_test_list))\n",
    "print(len(prediction_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Complaint',\n",
       " 'Routine Service',\n",
       " 'Routine Service',\n",
       " 'Routine Service',\n",
       " 'Complaint']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_list[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list \"mismatches\" that contains y_test indeces where y_test != predictions\n",
    "\n",
    "mismatches = []\n",
    "i = 0\n",
    "for i in range(len(y_test_list)):\n",
    "    if prediction_list[i] != y_test_list[i]:\n",
    "        mismatches.append(i)\n",
    "len(mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 12, 16, 21, 31, 37, 39, 50, 54, 62, 80, 82, 84, 89, 93, 95, 105, 106, 126, 129, 135, 137, 142, 146, 148, 157, 165, 168, 184, 186, 188, 193, 197, 204, 205, 209, 210, 214, 216, 228, 234, 237, 238, 242, 243, 249, 252, 256, 259, 261]\n"
     ]
    }
   ],
   "source": [
    "print(mismatches[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2480          Complaint\n",
       "2210    Routine Service\n",
       "1867    Routine Service\n",
       "Name: SR Type, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the dataframe row number using the \"mistmatches[]\" y_test indeces \n",
    "\n",
    "y_test[10:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Routine Service', 'Routine Service', 'Complaint'], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[10:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer had a gripper crash and cards were jammed. Customer reported a gripper crash and the cards were jammed. The TAS investigating this case worked with the customer on the phone to get the cards removed from the gripper. All cards that were loose were removed from the drawers. All the card drawers were checked to make sure the cards were sitting properly in the racks. The gripper was enabled. The next shift will ran QC and reported that they had some cards in centrifuge and had problems with gripper again. The cards were removed from centrifuge, QC was run and the customer was able to run patient samples. The customer was able to remove the cards from gripper and the centrifuge and restart analyzer. The root cause of the gripper error was that cards were sticking up higher than expected due to improper loading of cards and or closing the card drawer forcefully. A discussion took place with the customer to ensure that they understand proper loading of the cards and closing of the card drawers, so that the cards remain fully seated in the card rack.\n",
      "\n",
      "\n",
      "Actual:Complaint\n",
      "\n",
      "\n",
      "Predicted:Routine Service\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[2480]['Merged'])\n",
    "print('\\n')\n",
    "print(f\"Actual:{df.iloc[2480]['SR Type']}\")\n",
    "print('\\n')\n",
    "print(f\"Predicted:{predictions[10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index off y_test.iteritems()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1891, 1550, 1049, 2523, 156]\n",
      "['Complaint', 'Routine Service', 'Routine Service', 'Routine Service', 'Complaint']\n"
     ]
    }
   ],
   "source": [
    "# Add y_test Excel rows to a list\n",
    "# Add y_test_scores to a list\n",
    "\n",
    "y_test_rows = []\n",
    "y_test_scores = []\n",
    "\n",
    "for i,s in y_test.iteritems():\n",
    "    y_test_rows.append(i)\n",
    "    y_test_scores.append(s)\n",
    "    \n",
    "print(y_test_rows[0:5])\n",
    "print(y_test_scores[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add y_test vs. prediction mismatches to a list\n",
    "\n",
    "mismatch_list = []\n",
    "i = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test_scores[i] != predictions[i]:\n",
    "        mismatch_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 12, 16, 21, 31, 37, 39, 50, 54, 62, 80, 82, 84, 89, 93, 95, 105, 106, 126, 129, 135, 137, 142, 146, 148, 157, 165, 168, 184, 186, 188, 193, 197, 204, 205, 209, 210, 214, 216, 228, 234, 237, 238, 242, 243, 249, 252, 256, 259, 261, 262, 285, 307, 315, 319, 327, 328, 352, 356, 363, 364, 369, 370, 385, 388, 389, 392, 400, 402, 409, 415, 435, 438, 442, 443, 451, 455, 457, 466, 471, 484, 494, 499, 507, 518, 520, 523, 558, 559, 563, 575, 593, 599, 604, 608, 609, 611, 615, 616, 618]\n"
     ]
    }
   ],
   "source": [
    "# Get index of mismatches in y_test\n",
    "\n",
    "print(mismatch_list[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2480\n",
      "1867\n",
      "4787\n"
     ]
    }
   ],
   "source": [
    "# Connect the mismatched y_test indeces to the y_test Excel row number\n",
    "\n",
    "print(y_test_rows[10])\n",
    "print(y_test_rows[12])\n",
    "print(y_test_rows[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to evaluate mismatches\n",
    "\n",
    "#df.iloc[y_test_row[10]] \n",
    "#df.iloc[2480]\n",
    "\n",
    "def mismatch_eval(i):\n",
    "    print(df.iloc[y_test_rows[i]])\n",
    "    print('\\n')\n",
    "    print(df.iloc[y_test_rows[i]]['Merged'])\n",
    "    print('\\n')\n",
    "    print(f\"Actual: {df.iloc[y_test_rows[i]]['SR Type']}\")\n",
    "    print(f\"Prediction: {predictions[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Request Number                                          1-222097692\n",
      "SR Type                                                           Complaint\n",
      "Platform                                                             Erytra\n",
      "Merged                    Customer had a gripper crash and cards were ja...\n",
      "Name: 2480, dtype: object\n",
      "\n",
      "\n",
      "Customer had a gripper crash and cards were jammed. Customer reported a gripper crash and the cards were jammed. The TAS investigating this case worked with the customer on the phone to get the cards removed from the gripper. All cards that were loose were removed from the drawers. All the card drawers were checked to make sure the cards were sitting properly in the racks. The gripper was enabled. The next shift will ran QC and reported that they had some cards in centrifuge and had problems with gripper again. The cards were removed from centrifuge, QC was run and the customer was able to run patient samples. The customer was able to remove the cards from gripper and the centrifuge and restart analyzer. The root cause of the gripper error was that cards were sticking up higher than expected due to improper loading of cards and or closing the card drawer forcefully. A discussion took place with the customer to ensure that they understand proper loading of the cards and closing of the card drawers, so that the cards remain fully seated in the card rack.\n",
      "\n",
      "\n",
      "Actual: Complaint\n",
      "Prediction: Routine Service\n"
     ]
    }
   ],
   "source": [
    "mismatch_eval(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Evaluate mismatches, re-classify if required, and retrain model\n",
    "- Evaluate different modelsa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
